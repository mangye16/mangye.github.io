<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<HTML xml:lang="en" xmlns="http://www.w3.org/1999/xhtml">

<HEAD>

<META 
content="IE=7.0000" http-equiv="X-UA-Compatible">
<TITLE>Mang Ye's Homepage</TITLE>
<META name=keywords 
content="Mang Ye, Ye Mang, 叶茫, WHU, Wuhan University, HKBU, Hong Kong Baptist University">
<META content=text/html;charset=utf-8 http-equiv=Content-Type><LINK 
rel=stylesheet type=text/css 
href="files/jemdoc.css"><LINK rel="shortcut icon" 
href="files/rui.ico">
<META name=GENERATOR content="MSHTML 11.00.9600.17280"></HEAD>
<BODY>
<DIV id=layout-content>
<P>
<SCRIPT type=text/javascript>
<!--
// Toggle Display of BibTeX
function toggleBibtex(articleid) {
  var bib = document.getElementById(articleid);
  // Toggle 
    if(bib.style.display == "none") {
      bib.style.display = "";
    }
    else {
      bib.style.display = "none";
    }
}
-->
</SCRIPT>

<SCRIPT type=text/javascript>

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-40926388-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</SCRIPT>
</P>
<P></P>
<TABLE class=imgtable>
  <TBODY>
  <TR>
    <TD><IMG alt="alt text" src="files/yemang.jpg" 
      width=260> &nbsp;</TD>
    <TD align=left>
      <DIV id=toptitle>
      <H1><A href="http://www.comp.hkbu.edu.hk/~mangye/">Mang Ye</A> &nbsp; 叶 茫 
      </H1></DIV>
      <P> PhD Student, HKBU 
      <BR>supervised by <A href="http://www.comp.hkbu.edu.hk/v1/?page=profile&id=pcyuen">Prof. Pong C. Yuen</A>
      <BR>
      <BR>Email: <A href="mailto:mangye@comp.hkbu.edu.hk">mangye AT comp.hkbu.edu.hk</A> or <A href="mailto:mangye16@gmail.com">mangye16 AT gmail.com</A>
      <BR>RRS735, Department of Computer Science,  
      <BR>Hong Kong Baptist University, Hong Kong, China </P></TD></TR></TBODY>
</TABLE>&nbsp;&nbsp;


 [<a href="https://scholar.google.com/citations?user=j-HxRy0AAAAJ&hl=en">Google
          Scholar</a>] &nbsp;
[<a href="https://github.com/mangye16">GitHub</a>] &nbsp; 
[<a href="https://dblp.org/pers/hd/y/Ye:Mang">DBLP</a>]&nbsp;

      
    
<H2>News </H2>
<UL>
<LI>
   <P>2019.02, A survey of open-word person re-identification.  <A href="https://ieeexplore.ieee.org/document/8640834"> Link</A>.</P>   
</LI>
<LI>
   <P>2018.05, We have released the code of visible-thermal (cross-modality) person re-identification in AAAI 2018 and IJCAI 2018. <A href="https://github.com/mangye16/Visible-Thermal-Person-Re-Identification"> Code</A>.</P>   
</LI>
<LI>
   <P>2017.09, We have released the code of unsupervised video person re-identification in ICCV 2017. <A href="https://github.com/mangye16/dgm_re-id"> Code.</A></P>   
</LI>
<LI>
   <P><B> 2017.05, A comprehensive overview of <A href="https://wangzwhu.github.io/home/re_id_resources.html"> Re-id sources</A>. Provided by <A href="https://wangzwhu.github.io/home/"> Dr. Zheng Wang</A>. <font color="red">Updated !</font> </B> </P>   
</LI>
</UL>


<H2>Biography </H2>
<UL>
 <LI>
  <P> 2016-present, Ph.D student,  <A href="http://www.comp.hkbu.edu.hk/v1/">Department of Computer Science</A>,
  <A href="http://www.hkbu.edu.hk/">Hong Kong Baptist University</A>.
  </P>
 <LI>
  <P> 2018.07-2018.12, Visiting scholar at Columbia University working with <A href="http://www.ee.columbia.edu/~sfchang/">Shih-Fu Chang</A>.
  </P>
  <LI>
  <P> 2013-2016, MSc, <A href="http://mmap.whu.edu.cn"> MAP Group</A> supervised by <A href=" http://mmap.whu.edu.cn/personalpage/cliang_index.htm"> Chao Liang </A> and Jun Chen in <A href="http://en.multimedia.whu.edu.cn/">NERCMS</A>, <A href="http://en.whu.edu.cn/">Wuhan University</A>.
  </P>
  
   <LI>
   <P>2009-2013,	 B.E.,	<A href="http://eis.whu.edu.cn/">Electronic Information Engineering</A>,
  	<A href="http://en.whu.edu.cn/"> Wuhan University</A>, China.
   </P>
   <LI>
   <P>My research interests include computer vision and multimedia analysis, particularly person re-identification and instance retrieval in videos.
   </P>
  </LI></UL>
  
  

<H2>Selected Publications </H2>

<!-- 13 -->
 
<UL>
  <LI>
  <A >Unsupervised Embedding Learning via Invariant and Spreading Instance Feature </A>
  <BR><U>Mang Ye</U>, Xu Zhang, Pong C. Yuen, Shih-Fu Chang. 
  <BR> IEEE International Conference on Computer Vision and Pattern Recognition (<I>CVPR</I>), 2019. <BR>
  [<A href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Ye_Unsupervised_Embedding_Learning_via_Invariant_and_Spreading_Instance_Feature_CVPR_2019_paper.pdf"> PDF </A>]
  [<A href="javascript:toggleBibtex('cvpr19bib')"   target=_self> Bibtex</A>]
  [<A href="https://github.com/mangye16/Unsupervised_Embedding_Learning"> Code </A>]
  [<A href="https://arxiv.org/abs/1904.03436"> arXiv </A>]
  <DIV id=cvpr19bib class=blockcontent style="DISPLAY: none">
  <PRE>
@inproceedings{cvpr19unsupervised,
  title={Unsupervised Embedding Learning via Invariant and Spreading Instance Feature},
  author={Ye, Mang and Zhang, Xu and Yuen, Pong C. and Chang, Shih-Fu},
  booktitle={IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2019},
}
</PRE></DIV> 
</LI></UL> 
      
<!-- 12 -->
 
<UL>
  <LI>
  <A >Dynamic Graph Co-Matching for Unsupervised Video-based Person Re-Identification </A>
  <BR><U>Mang Ye</U>, Jiawei Li, Andy J Ma, Liang Zheng,  Pong C. Yuen. 
  <BR> IEEE Transactions on Image Processing (<I>TIP</I>), 2019. <BR>
  [<A href="https://ieeexplore.ieee.org/document/8611378"> PDF </A>]
  [<A href="javascript:toggleBibtex('tip19bib')"   target=_self> Bibtex</A>]  
  <font size="2.8", color="green">It's an extended version of our ICCV 17 paper. </font> 
  <DIV id=tip19bib class=blockcontent style="DISPLAY: none">
  <PRE>
@article{tip19dgm,
  title={Dynamic Graph Co-Matching for Unsupervised Video-based Person Re-Identification},
  author={Ye, Mang and Li, Jiawei and Ma, Andy J and Zheng, Liang and Yuen, Pong C},
  journal={IEEE Transactions on Image Processing (TIP)},
  year={2019},
  publisher={IEEE}
}
</PRE></DIV> 
</LI></UL> 


<!-- 11 -->
 
<UL>
  <LI>
  <A >Bi-directional Center-Constrained Top-Ranking for Visible Thermal Person Re-Identification </A>
  <BR><U>Mang Ye</U>, Xiangyuan Lan, Zheng Wang,  Pong C. Yuen. 
  <BR> IEEE Transactions on Information Forensics and Security (<I>TIFS</I>), 2019. <BR>
  [<A href="https://ieeexplore.ieee.org/document/8732420">  PDF </A>]
  [<A href="javascript:toggleBibtex('tifs19bib')"   target=_self> Bibtex</A>]  
  <font size="2.8", color="green">It's an extended version of our IJCAI 18 paper. </font> 
  <DIV id=tifs19bib class=blockcontent style="DISPLAY: none">
  <PRE>
@article{tifs19vtreid,
  title={Bi-directional Center-Constrained Top-Ranking for Visible Thermal Person Re-Identification},
  author={Ye, Mang and Lan, Xiangyuan and Wang, Zheng and Yuen, Pong C},
  journal={IEEE Transactions on Information Forensics and Security (TIFS)},
  year={2019},
  publisher={IEEE}
}
</PRE></DIV> 
</LI></UL> 
<!-- 10 -->
<UL>
  <LI>
  <A >A Survey of Open-World Person Re-identification </A>
  <BR>Qingming Leng, <U>Mang Ye</U>, Qi Tian. 
  <BR> IEEE Transactions on Circuits and Systems for Video Technology (<I>TCSVT</I>), 2019. <BR>
  [<A href="files/tcsvt19_survey.pdf"> PDF </A>]
  [<A href="javascript:toggleBibtex('tcsvt19bib')"   target=_self> Bibtex</A>]  
  <DIV id=tcsvt19bib class=blockcontent style="DISPLAY: none">
  <PRE>
@article{tcsvt19survey,
  title={A Survey of Open-World Person Re-identification},
  author={Leng, Qingming and Ye, Mang and Tian, Qi},
  journal={IEEE Transactions on Circuits and Systems for Video Technology},
  year={2019},
  publisher={IEEE}
}
</PRE></DIV> 
</LI></UL> 
          
<!-- 9 -->
 
<UL>
  <LI>
  <A >Robust Anchor Embedding for Unsupervised Video Person Re-Identification in the Wild </A>
  <BR><U>Mang Ye</U>,  Xiangyuan Lan, Pong C. Yuen. 
  <BR> European Conference on Computer Vision (<I>ECCV</I>), 2018. <BR> 
  [<A href="files/RACE.pdf"> PDF </A>]    
  [<A href="javascript:toggleBibtex('eccv18_abstract')" target=_self> Abstract</A>] 
  [<A href="javascript:toggleBibtex('eccv18bib')"   target=_self> Bibtex</A>]   
  <DIV id=eccv18_abstract class=blockcontent style="DISPLAY: none">
  <TABLE class=imgtable>
    <TBODY>
    <TR>
      <TD>
        <P style="FONT-SIZE: 16px">This paper addresses the scalability and robustness issues of estimating labels from imbalanced unlabeled data for unsupervised video-based person re-identification (re-ID). To achieve it, we propose a novel Robust AnChor Embedding (RACE) framework via deep feature representation learning for large-scale unsupervised video re-ID. Within this framework, anchor sequences representing different persons are firstly selected to formulate an anchor graph which also initializes the CNN model to get discriminative feature representations for later label estimation. To accurately estimate labels from unlabeled sequences with noisy frames, robust anchor embedding is introduced based on the regularized affine hull. Efficiency is ensured with $k$NN anchors embedding instead of the whole anchor set under manifold assumptions. After that, a robust and efficient top-$k$ counts label prediction strategy is proposed to predict the labels of unlabeled image sequences. With the newly estimated labeled sequences, the unified anchor embedding framework enables the feature learning process to be further facilitated. Extensive experimental results on the large-scale dataset show that the proposed method outperforms existing unsupervised video re-ID methods.
</P></TD></TR></TBODY></TABLE></DIV>
  <DIV id=eccv18bib class=blockcontent style="DISPLAY: none">
  <PRE>
@inproceedings{eccv18race,
  title={Robust Anchor Embedding for Unsupervised Video Person Re-Identification in the Wild},
  author={Ye, Mang and Lan, Xiangyuan and Yuen, Pong C.},
  booktitle={ECCV},
  year={2018},
}
</PRE></DIV> 
</LI></UL> 



<!-- 8 -->

<UL>
  <LI>
  <A >Visible Thermal Person Re-Identification via Dual-Constrained Top-Ranking </A>
  <BR><U>Mang Ye</U>, Zheng Wang, Xiangyuan Lan, Pong C. Yuen. 
  <BR>27th International Joint Conference on Artificial Intelligence (<I>IJCAI</I>), 2018. <BR>
  [<A href="https://www.ijcai.org/proceedings/2018/0152.pdf"> PDF </A>]    
  [<A href="javascript:toggleBibtex('ijcai18_abstract')" target=_self> Abstract</A>] 
  [<A href="javascript:toggleBibtex('ijcai18bib')"   target=_self> Bibtex</A>]
  [<A href="https://github.com/mangye16/Visible-Thermal-Person-Re-Identification"> Code </A>]  
  <DIV id=ijcai18_abstract class=blockcontent style="DISPLAY: none">
  <TABLE class=imgtable>
    <TBODY>
    <TR>
      <TD>
        <P style="FONT-SIZE: 16px">Cross-modality person re-identification between the thermal and visible domains is extremely important for night-time surveillance applications. Existing works in this filed mainly focus on learning sharable feature representations to handle the cross-modality discrepancies. However, besides the cross-modality discrepancy caused by different camera spectrums, visible thermal person re-identification also suffers from large cross-modality and intra-modality variations caused by different camera views and human poses. In this paper, we propose a dual-path network with a novel bi-directional dual-constrained top-ranking loss to learn discriminative feature representations. It is advantageous in two aspects: 1) end-to-end feature learning directly from the data without extra metric learning steps, 2) it simultaneously handles the cross-modality and intra-modality variations to ensure the discriminability of the learnt representations. Meanwhile, identity loss is further incorporated to model the identity-specific information to handle large intra-class variations. Extensive experiments on two datasets demonstrate the superior performance compared to the state-of-the-arts.
</P></TD></TR></TBODY></TABLE></DIV>
  <DIV id=ijcai18bib class=blockcontent style="DISPLAY: none">
  <PRE>
@inproceedings{ijcai18vtreid,
  title={Visible Thermal Person Re-Identification via Dual-Constrained Top-Ranking},
  author={Ye, Mang and Wang, Zheng and Lan, Xiangyuan and Yuen, Pong C.},
  booktitle={IJCAI},
  year={2018},
}
</PRE></DIV>
</LI></UL> 


<!-- 7 -->

<UL>
  <LI>
  <A> Cascaded SR-GAN for Scale-Adaptive Low Resolution Person Re-identification </A>
  <BR>Zheng Wang, <U>Mang Ye</U>, Fan Yang, Xiang Bai, Shin’ichi Satoh.
  <BR>27th International Joint Conference on Artificial Intelligence (<I>IJCAI</I>), 2018. <BR> 
  [<A href="https://www.ijcai.org/proceedings/2018/0541.pdf"> PDF </A>]    
  [<A href="javascript:toggleBibtex('ijcai18_abstract2')" target=_self> Abstract</A>] 
  [<A href="javascript:toggleBibtex('ijcai18bib2')"   target=_self> Bibtex</A>]
  [<A href="https://github.com/wangzwhu/CSR-GAN"> Code </A>] 
  <DIV id=ijcai18_abstract2 class=blockcontent style="DISPLAY: none">
  <TABLE class=imgtable>
    <TBODY>
    <TR>
      <TD>
        <P style="FONT-SIZE: 16px">Person re-identification (REID) is an important task in video surveillance and forensics applications. Most of previous approaches are based on a key assumption that all person images have uniform and sufficiently high resolutions. Actually, various low-resolutions and scale mismatching always exist in open world REID. We name this kind of problem as Scale-Adaptive Low Resolution Person Re-identification (SALR-REID). The most intuitive way to address this problem is to increase various low-resolutions (not only low, but also with different scales) to a uniform high-resolution. SRGAN is one of the most competitive image superresolution deep networks, designed with a fixed upscaling factor. However, it is still not suitable for SALR-REID task, which requires a network not only synthesizing high-resolution images with different upscaling factors, but also extracting discriminative image feature for judging person’s identity. (1) To promote the ability of scale-adaptive upscaling, we cascade multiple SRGANs in series. (2) To supplement the ability of image feature representation, we plug-in a reidentification network. With a unified formulation, a Cascaded Super-Resolution GAN (CSRGAN) framework is proposed. Extensive evaluations on two simulated datasets and one public dataset demonstrate the advantages of our method over related state-of-the-art methods.
</P></TD></TR></TBODY></TABLE></DIV>
  <DIV id=ijcai18bib2 class=blockcontent style="DISPLAY: none">
  <PRE>
@inproceedings{wang2018cascaded,
  title={Cascaded SR-GAN for Scale-Adaptive Low Resolution Person Re-identification.},
  author={Wang, Zheng and Ye, Mang and Yang, Fan and Bai, Xiang and Satoh, Shin'ichi},
  booktitle={IJCAI},
  year={2018}
}
</PRE></DIV>
</LI></UL> 


<!-- 6 -->

<UL>
  <LI>
  <A>Hierarchical Discriminative Learning for Visible Thermal Person Re-Identification </A>
  <BR><U>Mang Ye</U>, Xiangyuan Lan, Jiawei Li, Pong C. Yuen. 
  <BR>Thirty-Second AAAI Conference on Artificial Intelligence (<I>AAAI</I>), 2018. <BR> 
  [<A href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16734/16350"> PDF</A>]    
  [<A href="javascript:toggleBibtex('aaai18_abstract')" target=_self> Abstract</A>] 
  [<A href="javascript:toggleBibtex('aaai18bib')"   target=_self> Bibtex</A>]
  [<A href="https://github.com/mangye16/Visible-Thermal-Person-Re-Identification"> Code </A>] 
  
  <DIV id=aaai18_abstract class=blockcontent style="DISPLAY: none">
  <TABLE class=imgtable>
    <TBODY>
    <TR>
    <!--
      <TD><A href="files/aaai18_framework.jpg"> 
      <IMG alt="alt text" src="files/aaai18_framework.jpg" 
        width=400></A></TD>
        -->
      <TD>
        <P style="FONT-SIZE: 16px">Person re-identification is widely studied in visible spectrum, where all the person images are captured by visible cameras. However, visible cameras may not capture valid appearance information under poor illumination conditions, e.g, at night. In this case, thermal camera is superior since it is less dependent on the lighting by using infrared light to capture the human body. To this end, this paper investigates a cross-modal re-identification problem, namely visible-thermal person re-identification (VT-REID). Existing cross-modal matching methods mainly focus on modeling the cross-modality discrepancy, while VT-REID also suffers from cross-view variations caused by different camera views. Therefore, we propose a hierarchical cross-modality matching model by jointly optimizing the modality-specific and modality-shared metrics. The modality-specific metrics transform two heterogenous modalities into a consistent space that modality-shared metric can be subsequently learnt. Meanwhile, the modality-specific metric compacts features of the same person within each modality to handle the large intra-modality intra-person variations (e.g. viewpoints, pose). Additionally, an improved two-stream CNN network is presented to learn the multi-modality sharable feature representations. Identity loss and contrastive loss are integrated to enhance the discriminability and modality-invariance with partially shared layer parameters. Extensive experiments illustrate the effectiveness and robustness of the proposed method
</P></TD></TR></TBODY></TABLE></DIV>
  <DIV id=aaai18bib class=blockcontent style="DISPLAY: none">
  <PRE>
@inproceedings{aaai18vtreid,
  title={Hierarchical Discriminative Learning for Visible Thermal Person Re-Identification},
  author={Ye, Mang and Lan, Xiangyuan and Li, Jiawei and Yuen, Pong C.},
  booktitle={AAAI},
  year={2018},
}
</PRE></DIV>
</LI></UL> 


 
<!-- 5 -->

<UL>
  <LI>
  <A>Dynamic Label Graph Matching for Unsupervised Video Re-Identification </A>
  <BR><U>Mang Ye</U>, Andy J Ma, Liang Zheng, Jiawei Li, Pong C. Yuen. 
  <BR>IEEE International Conference on Computer Vision (<I>ICCV</I>), 2017.  <BR>
  [<A href="https://arxiv.org/pdf/1709.09297.pdf">PDF</A>]    
  [<A href="javascript:toggleBibtex('iccv_abstract')" target=_self>Abstract</A>] 
  [<A href="javascript:toggleBibtex('iccv17bib')"   target=_self>Bibtex</A>] 
  [<A href="https://github.com/mangye16/dgm_re-id"> Code </A>] 
  [<A href="https://arxiv.org/abs/1709.09297"> arXiv </A>]
  <BR> <font size="2.8", color="green"> <I> For updated results under standard evaluation protocol on MARS, please refer to the <A href="https://arxiv.org/abs/1709.09297"> arXiv</A> version and <A href="https://github.com/mangye16/dgm_re-id">Github</A>. <BR></font></I>   
 
   <DIV id=iccv_abstract class=blockcontent style="DISPLAY: none">
  <TABLE class=imgtable>
    <TBODY>
    <TR>
      <TD><A href="files/iccv17dgm.jpg"> 
      <IMG alt="alt text" src="files/iccv17dgm.jpg" 
        width=400></A></TD>
      <TD>
        <P style="FONT-SIZE: 16px">Label estimation is an important component in an unsupervised person re-identification (re-ID) system. This paper focuses on cross-camera label estimation, which can be subsequently used in feature learning to learn robust re-ID models. Specifically, we propose to construct a graph for samples in each camera, and then graph matching scheme is introduced for cross-camera labeling association. While labels directly output from existing graph matching methods may be noisy and inaccurate due to significant crosscamera variations, this paper propose a dynamic graph matching (DGM) method. DGM iteratively updates the image graph and the label estimation process by learning a better feature space with intermediate estimated labels. DGM is advantageous in two aspects: 1) the accuracy of estimated labels is improved significantly with the iterations; 2) DGM is robust to noisy initial training data. Extensive experiments conducted on three benchmarks including the large-scale MARS dataset show that DGM yields competitive performance to fully supervised baselines, and outperforms competing unsupervised learning methods.
</P></TD></TR></TBODY></TABLE></DIV>
  <DIV id=iccv17bib class=blockcontent style="DISPLAY: none">
  <PRE>
@inproceedings{iccv17dgm,
  title={Dynamic Label Graph Matching for Unsupervised Video Re-Identification},
  author={Ye, Mang and Ma, Andy J and Zheng, Liang and Li, Jiawei and Yuen, Pong C.},
  booktitle={ICCV},
  year={2017},
}
</PRE></DIV>
</LI></UL> 




<!-- 4 -->
<UL>
  <LI>
  <A>Person Re-identification via Ranking Aggregation of Similarity Pulling and Dissimilarity Pushing </A>
  <BR><U>Mang Ye</U>, Chao Liang, <A href="http://research.nii.ac.jp/~yiyu/"> Yi Yu </A>, Zheng Wang, Qingming Leng, <A href="http://graphvision.whu.edu.cn/"> Chunxia Xiao </A>, Jun Chen, Ruimin Hu. 
  <BR>IEEE Transactions on Multimedia (<I>TMM</I>), 2016.  <BR>
  [<A href="https://ieeexplore.ieee.org/abstract/document/7557057">PDF</A>]    
  [<A href="javascript:toggleBibtex('tmm_abstract')" target=_self>Abstract</A>] 
  [<A href="javascript:toggleBibtex('tmm16bib')"   target=_self>Bibtex</A>] 
  [<A href="https://github.com/mangye16/dsra"   target=_self>Code</A>] 
  [<A href="javascript:toggleBibtex('tmm16video')"   target=_self>Video</A>] 
 
   <DIV id=tmm_abstract class=blockcontent style="DISPLAY: none">
  <TABLE class=imgtable>
    <TBODY>
    <TR>
      <TD><A href="files/tmm16_ym.jpg"> 
      <IMG alt="alt text" src="files/tmm16_ym.jpg" 
        width=400></A></TD>
      <TD>
        <P style="FONT-SIZE: 16px">Person re-identification is a key technique to match different persons observed in non-overlapping camera views.
Many researchers treat it as a special object retrieval problem, where ranking optimization plays an important role. Existing ranking optimization methods mainly utilize similarity relationship between the probe and gallery images to optimize the original ranking list, but seldom consider the important dissimilarity relationship. In this paper, we propose to use both similarity and dissimilarity cues in a ranking optimization framework for person re-identification. Its core idea is that the true match should not only be similar to those strongly similar galleries of the probe, but also be dissimilar to those strongly dissimilar galleries of the probe. Furthermore, motivated by the philosophy of multiview verification, a ranking aggregation algorithm is proposed to enhance the detection of similarity and dissimilarity based on the following assumption: the true match should be similar to the probe in different baseline methods. In other words, if a gallery blue image is strongly similar to the probe in one method, while simultaneously, strongly dissimilar to the probe in another method, it will probably be a wrong match of the probe. Extensive experiments conducted on public benchmark datasets and comparisons with different baseline methods have shown the great superiority of the proposed ranking optimization method.
</P></TD></TR></TBODY></TABLE></DIV>
  <DIV id=tmm16bib class=blockcontent style="DISPLAY: none">
  <PRE>
@inproceedings{ye2016rank,
  title={Person Re-identification via Ranking Aggregation of Similarity Pulling and Dissimilarity Pushing},
  author={Ye, Mang and Liang, Chao and Yu, Yi and et al.},
  booktitle={IEEE Transactions on Multimedia (TMM)},
  year={2016},
  organization={IEEE}
}
</PRE></DIV>
</LI></UL>


<!-- 3 -->
<UL>
  <LI>
  <A>Ranking Optimization for Person Re-identification via Similarity and Dissimilarity </A>
  <BR><U>Mang Ye</U>, Chao Liang, Zheng Wang, Qingming Leng, Jun Chen. 
  <BR>ACM international conference on Multimedia (<I>MM</I>), 2015. <BR>
  [<A href="https://dl.acm.org/citation.cfm?id=2806326">PDF</A>]   
  [<A href="javascript:toggleBibtex('ymmm_abstract')" target=_self>Abstract</A>] 
  [<A href="javascript:toggleBibtex('ymmm15')"   target=_self>Bibtex</A>] 
 
  <DIV id=ymmm_abstract class=blockcontent style="DISPLAY: none">
  <TABLE class=imgtable>
    <TBODY>
    <TR>
      <TD><A href="files/mm2015_ym.jpg"> 
      <IMG alt="alt text" src="files/mm2015_ym.jpg" 
        width=400></A></TD>
      <TD>
        <P style="FONT-SIZE: 16px">Person re-identification is a key technique to match different persons observed in non-overlapping camera views. Many researchers treat it as a special object retrieval problem,
where ranking optimization plays an important role. Existing ranking optimization methods utilize the similarity relationship between the probe and gallery images to optimize the original ranking list in which dissimilarity relationship is seldomly investigated. In this paper, we propose to use both similarity and dissimilarity cues in a ranking optimization framework for person re-identification. Its core idea is based on the phenomenon that the true match should not only be similar to the strong similar samples of the probe but also dissimilar to the strong dissimilar samples. Extensive experiments have shown the great superiority of the proposed ranking optimization method.
</P></TD></TR></TBODY></TABLE></DIV>
  <DIV id=ymmm15 class=blockcontent style="DISPLAY: none">
  <PRE>
@inproceedings{ye2015ranking,
  title={Ranking Optimization for Person Re-identification via Similarity and Dissimilarity},
  author={Ye, Mang and Liang, Chao and Wang, Zheng and Leng, Qingming and Chen, Jun},
  booktitle={Proceedings of the 23rd ACM international conference on Multimedia},
  pages={1239--1242},
  year={2015},
  organization={ACM}
}
</PRE></DIV>
</LI></UL>

<!-- 2 -->
<UL>
  <LI>
  <A>Specific Person Retrieval via Incomplete Text Description </A>
  <BR><U>Mang Ye</U>, Chao Liang, Zheng Wang, Qingming Leng, Jun Chen, Jun Liu. 
  <BR>International Conference on Multimedia Retrieval (<I>ICMR</I>), 2015. <BR>
  [<A href="https://dl.acm.org/citation.cfm?id=2749347">PDF</A>]   
  [<A href="javascript:toggleBibtex('ymicmr_abstract')" target=_self>Abstract</A>] 
  [<A href="javascript:toggleBibtex('ymicmr14')"   target=_self>Bibtex</A>] 
 
  <DIV id=ymicmr_abstract class=blockcontent style="DISPLAY: none">
  <TABLE class=imgtable>
    <TBODY>
    <TR>
      <TD><A href="files/icmr_ym.jpg"> 
      <IMG alt="alt text" src="files/icmr_ym.jpg" 
        width=400></A></TD>
      <TD>
        <P style="FONT-SIZE: 16px">Searching for specific persons from surveillance videos captured by different cameras, is a key yet under-addressed
challenge in multimedia system. Related person retrieval works mainly focus on searching person by visual appearance, known as person re-identification. However, the initial
visual image may not be available in some practical applications. For example, the criminal is described by a text description indirectly, “A young woman wearing a red casual with a backpack”, the traditional methods can not conquer
this issue. Based on a set of pre-defined attributes that the text description query can be transformed to an attribute vector, thus can be used to retrieval in the gallery set. And yet, the user-provided attributes are sometimes incomplete.
This new issue is defined as Specific Person Retrieval via Incomplete Text Description. In this paper, we conduct a specific attribute completion to enrich the original text query
and generate a more expressive attribute vector. Then, a pairwise-based metric learning is introduced for completed attribute vectors. Extensive experiments conducted on two
benchmark datasets have shown our superior performance.
</P></TD></TR></TBODY></TABLE></DIV>
  <DIV id=ymicmr14 class=blockcontent style="DISPLAY: none">
  <PRE>
 @inproceedings{icmr_ym,
 title = {Specific Person Retrieval via Incomplete Text Description},
 author={Ye, Mang and Chao, Liang and Zheng, Wang and et al.},
 booktitle = {International Conference on Multimedia Retrieval (ICMR)},
 year = {2015},
 month = {June},
 address = {Shanghai, China}
}
</PRE></DIV>
</LI></UL>

<!-- 1 -->
<UL>
  <LI>
  <A>Coupled-view Based Ranking Optimization for Person Re-identification </A>
  <BR><U>Mang Ye</U>, Jun Chen, Qingming Leng, Chao Liang, Zheng Wang and Kaimin Sun. 
  <BR>International Conference on Multimedia Modelling (<I>MMM</I>), 2015. Oral. <BR>
  [<A href="https://link.springer.com/chapter/10.1007/978-3-319-14445-0_10">PDF</A>]   
  [<A href="javascript:toggleBibtex('ymmmm14_abstract')" target=_self>Abstract</A>] 
  [<A href="javascript:toggleBibtex('ymmmm14')"   target=_self>Bibtex</A>] 
 
  <DIV id=ymmmm14_abstract class=blockcontent style="DISPLAY: none">
  <TABLE class=imgtable>
    <TBODY>
    <TR>
      <TD><A href="files/project_ym_mmm2015.jpg"> 
      <IMG alt="alt text" src="files/project_ym_mmm2015.jpg" 
        width=400></A></TD>
      <TD>
        <P style="FONT-SIZE: 16px">Person re-identification aims to match different persons observed in non-overlapping camera views. Researchers have proposed many person descriptors based on global or local descriptions, while both of them have achieved satisfying matching results, however, their ranking lists usually vary a lot for the same query person. These motivate us to investigate an approach to aggregate them to optimize the original matching results. In this paper, we proposed a coupled-view based ranking optimization method through cross KNN rank aggregation and graph-based re-ranking to revise the original ranking lists. Its core assumption is that the images of the same person should share the similar visual appearance in both global and local views. Extensive experiments on two datasets show the superiority of our proposed method with an average improvement of 20-30% over the state-of-the-art methods at CMC@1.
</P></TD></TR></TBODY></TABLE></DIV>
  <DIV id=ymmmm14 class=blockcontent style="DISPLAY: none">
  <PRE>
 @inproceedings{ye2015,
 title = {Coupled-view Based Ranking Optimization for Person Re-identification},
 author={Ye, Mang and Chen, Jun and Leng, Qingming and et al.},
 booktitle = {International Conference on Multimedia Modeling (MMM)},
 year = {2015},
 month = {Januray},
 address = {Sdyney, Australia}
}
</PRE></DIV>
</LI></UL>

<!-- 3 -->
<UL>
  <LI><A> WHU-NERCMS at TREVCID 2015:Instance search task </A>
  <BR> Lei Yao, <U>Mang Ye</U>, Dongjing Liu, Rui Shao, Tao Liu, Jun Liu, Zheng Wang, Chao Liang. 
  <BR><I>Participant Notebook Paper , TRECVID</I>, 2015. (Ranked 4th /31 teams)<BR>
  [<A href="http://www-nlpir.nist.gov/projects/tvpubs/tv15.papers/nercms.pdf">PDF</A>]
  [<A href="javascript:toggleBibtex('trecvid2015_abstract')" target=_self>Abstract</A>] 
  [<A href="http://www-nlpir.nist.gov/projects/tvpubs/tv.pubs.15.org.html">Link</A>]
  [<A href="http://news.whu.edu.cn/info/1002/44564.htm">Media Report</A>]
    <DIV id=trecvid2015_abstract class=blockcontent style="DISPLAY: none">
  <TABLE class=imgtable>
    <TBODY>
    <TR>
      <TD><A href="files/trecvid15.jpg"> 
      <IMG alt="alt text" src="files/trecvid15.jpg" 
        width=400></A></TD>
      <TD>
        <P style="FONT-SIZE: 16px">This paper introduces our work at the automatic instance search task of TRECVID 2015. The purpose of this task is to search specific
targets in a large-scale video database. The key problems this year we are concerned about includes: 1. How to improve traditional BoW
models; 2. How to improve retrieval precision with cross-mode information as an auxiliary to traditional visual features; 3. How to optimize
the initial retrieval results. Correspondingly, our work is divided into three parts: First part is object retrieval with visual features based on
BoW model. In this part, Bow model is augmented by Query Adaptive Similarity Measure. Second part is object retrieval with textual information.
We adopt caption information and plot information of the series EastEnders in its official websites . Third part includes several fusion and optimization strategies adopted to improve the initial results. The
main improvement in the strategies is Query Expansion With Adjacent Shots, which aims to retrieve more precisely in the adjacent shots of initial top-k results.
</P></TD></TR></TBODY></TABLE></DIV>
  
</LI></UL>

<!-- 2 -->
<UL>
  <LI>
  <A> WHU-NERCMS at TREVCID 2014:Instance search task </A>
  <BR> <U>Mang Ye</U>, Bingyue Huang, Lei Yao, et al. <BR><I>Participant Notebook Paper , TRECVID</I>, 2014.<BR>
  [<A href="files/nercms14.pdf">PDF</A>]
  [<A href="javascript:toggleBibtex('trecvid2014_abstract')" target=_self>Abstract</A>] 
  [<A href="http://www-nlpir.nist.gov/projects/tvpubs/tv.pubs.14.org.html">Link</A>]
    <DIV id=trecvid2014_abstract class=blockcontent style="DISPLAY: none">
  <TABLE class=imgtable>
    <TBODY>
    <TR>
      <TD><A href="files/trecvid2014.jpg"> 
      <IMG alt="alt text" src="files/trecvid2014.jpg" 
        width=400></A></TD>
      <TD>
        <P style="FONT-SIZE: 16px">This paper introduces our work at the automatic instance
search task of TRECVID 2014. Our work is divided into two parts: First
part is object retrieval based on BOW. Specially, we extract feature
histogram of frames through general BoW. We adopt similarity measure
method to compare the probe and gallery shots, then we obtain the initial
ranking results; Second, several optimization strategies are adopted to
improve the initial results.
</P></TD></TR></TBODY></TABLE></DIV>
  
</LI></UL>


<H2>Awards </H2>
<UL>
  <LI>  <P> 2019.04, Yakun Scholarship Scheme for Mainland Postgraduate Students</P>
  <LI>  <P> 2016-2018,    <A href = "http://www.comp.hkbu.edu.hk/v1/?pid=48"> Computer Science Department RPg Performance Award </A></P>
  <LI>  <P> 2016-2019,  <A href = "https://cerg1.ugc.edu.hk/hkpfs/index.html"> Hong Kong PhD Fellowship</A> </P>
  <LI>  <P> 2016.01,    Academic Breakthrough Prize awarded by NERCMS </P>
  <LI>  <P> 2015.10,    National Scholarship  </P>
  <LI>  <P> 2015.06,    <A href = "http://www.google.cn/intl/en/university/student/scholarship-recipients.html"> Google Excellence Scholarship </A> </P>
  <LI>  <P> 2014.10,    National Scholarship  </P>
  <LI>  <P> 2014.08,    <A href = "http://mmap.whu.edu.cn/national2014/">3rd Prize </A>in 1st National Graduate Contest on Smart-City Technology and Creative Design</P>
</UL>

<H2>Competitions </H2>
<UL>
 <LI>  <P>2015.5-2015.8, TRECVID 2015: Instance search task. I worked as a mentor. Our final results ranked 4th over 31 participants.</P>
  <LI>  <P>2014.6-2014.9,  <A href = "http://mmap.whu.edu.cn/national2014/">Multi-camera Tracking via Person Re-identification</A>, National Graduate Contest on Smart-City Technology and Creative Design.</P>
  <LI>  <P>2014.5-2014.8, TRECVID 2014: Instance search task. I worked as a group leader.</P>
</UL>

<H2>Professional Services </H2>
Invited PC Member/Reviewer for conferences:<br>
<UL>
<LI>  <P> ICCV 2019, CVPR 2018 2019, IJCAI 2017 2018, AAAI 2018, ICPR 2018, ACCV 2018, BMVC 2019</P>
<LI>  <P> IJCAI 2019 Session Chair</P>
</UL>
Invited Reviewer for journals: <br>
<UL>
  <LI>  <P> IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</P>
  <LI>  <P> IEEE  Transactions on Image Processing (TIP) </P>
  <LI>  <P> IEEE Transactions on Multimedia (TMM)</P>
  <LI>  <P> Pattern Recognition (PR)</P>
  <LI>  <P> IEEE Access </P>
  <LI>  <P> Neurocomputing <B>(Outstanding Reviewer Award)</B></P>
  <LI>  <P> Pattern Recognition Letters (PRL) <B>(Outstanding Reviewer Award)</B></P>
</UL>

<H2>Teaching Assistant </H2>
<UL>
  <LI>  <P> COMP4005, Information Systems Theory, Methodology and Architecture [2017-18 S1] </P>
  <LI>  <P> COMP7400, Financial Analysis and Decision Making [2016-17-18 S2]</P>
  <LI>  <P> COMP7800, Analytic Models in IT Management [2016-17 S1]</P>
  <LI>  <P> COMP2005, Business in the IT Context [2016-17 S1] </P>
  
</UL>
 
<!-- 
<H2>Collaborators </H2>
<UL>
<LI> <P> <A href="https://wangzwhu.github.io/home/">Zheng Wang</A>, at NII</P>
<LI> <P> <A href="http://isee.sysu.edu.cn/~majh/">Andy J Ma</A>, at SYSU</P>
<LI> <P> <A href="http://www.liangzheng.com.cn/">Liang Zheng</A>, at UTS</P>
<LI> <P> <A href="http://research.nii.ac.jp/~yiyu/">Yi Yu</A>, at NII</P>
<LI> <P> <A href="http://www.comp.hkbu.edu.hk/~mengpang/">Meng Pang</A>, My friend at HKBU</P> 
<LI> <P> <A href="http://mmap.whu.edu.cn/">MMAP Group </A></P>-->

</UL>
<P> <A href="http://en.whu.edu.cn/"> <IMG alt="alt text" src="files/whulogo.jpg" height = 30 width=120></A> &nbsp;<A href="http://cs.whu.edu.cn/cs2011/"> <IMG alt="alt text" src="files/whu-cslogo.jpg" height = 30 width=120></A>&nbsp; <A href="http://multimedia.whu.edu.cn/"> <IMG alt="alt text" src="files/whu-nercmslogo.jpg" height = 30 width=120></A> &nbsp;<A href="http://www.hbshgzx.com/"> <IMG alt="alt text" src="files/huangganglogo.jpg" height = 30 width=60></A> </P>

<P><a href="http://www.easycounter.com/">
<img src="//www.easycounter.com/counter.php?yemang"
border="0" alt="HTML Hit Counter"></a>
<br><a href="http://www.easycounter.com/">Unique visitors since Sep 2017</a></P>       
<DIV id=footer>
</DIV></DIV></BODY>
<DIV align=center>
<DIV id=clustrmaps-widget></DIV>
<NOSCRIPT></NOSCRIPT></DIV></HTML>
